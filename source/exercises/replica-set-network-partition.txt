Exercise: Elections in Failover Scenarios
-----------------------------------------

- We have learned about electing a primary in replica sets
- Let's look at some scenarios in which failover might be necessary.


Scenario A: 3 Data Nodes in 1 DC
--------------------------------

Which secondary will become the new primary?

.. figure:: /figures/replica-set-1dc-primary-down.png
   :width: 700px
   :align: center

.. only:: instructor

   .. note::

      - It depends on the priorities of the secondaries.
      - And on the optime.


Scenario B: 3 Data Nodes in 2 DCs
---------------------------------

Which member will become primary following this type of network partition?

.. figure:: /figures/replica-set-3nodes-2dc-network-partition.png
   :width: 700px
   :align: center

.. only:: instructor

   .. note::

      - The current primary is likely to remain primary.
      - It probably has the highest priority.
      - If DC2 fails, we still have a primary.
      - If DC1 fails, we won't have a primary automatically. The remaining node in DC2 needs to be manually promoted by reconfiguring the replica set.


Scenario C: 4 Data Nodes in 2 DCs
---------------------------------

What happens following this network partition?

.. figure:: /figures/replica-set-4nodes-2dc-cant-elect.png
   :width: 700px
   :align: center

.. only:: instructor

   .. note::

      - We enter a state with no primary.
      - Each side of the network partition has only 2 votes (not a majority).
      - All the servers assume secondary status.
      - This is avoidable.
      - One solution is to add another member to the replica set.
      - If another data node can not be provisioned, MongoDB has a special alternative called an arbiter that requires minimal resources.
      - An arbiter is a ``mongod`` instance without data and performs only heartbeats, votes, and vetoes.


Scenario D: 5 Nodes in 2 DCs
----------------------------

The following is similar to Scenario C, but with the addition of an arbiter in Data Center 1. What happens here?

.. figure:: /figures/replica-set-4nodes-1arbiter-2datacenters.png
   :width: 700px
   :align: center

.. only:: instructor

   .. note::

      - The current primary is likely to remain primary.
      - The arbiter helps ensure that the primary can reach a majority of the replica set.


Scenario E: 3 Data Nodes in 3 DCs
---------------------------------

- What happens here if any one of the nodes/DCs fail? 
- What about recovery time?

.. figure:: /images/replica-set-3nodes-1-per-dc.png
   :width: 700px
   :align: center

.. only:: instructor

   .. note::

      - The intent is to explain the advantage of deploying to 3 DCs - it's the minimum number of DCs in order for MongoDB to automatically failover if any one DC fails. This is generally what we recommend to customers in our consult and health check reports, though many continue to use 2 DCs due to costs and legacy reasons.
      - To have automated failover in the event of single DC level failure, there must be at least 3 DCs. Otherwise the DC with the minority of nodes must be manually reconfigured.
      - One of the data nodes can be replaced by an arbiter to reduce costs.


Scenario F: 5 data nodes in 3 DCs
---------------------------------

What happens here if any one of the nodes/DCs fail? What about recovery time?

.. figure:: /figures/replica-set-three-data-centers-votes.png
   :width: 700px
   :align: center

.. only:: instructor

   .. note::

      - Adds another data node to each "main" DC to reduce typically slow and costly cross DC network traffic if an initial sync or similar recovery is needed, as the recovering node can pull from a local replica instead. 
      - Depending on the data sizes, operational budget, and requirements, this can be overkill.
      - The data node in DC3 can be replaced by an arbiter to reduce costs.

