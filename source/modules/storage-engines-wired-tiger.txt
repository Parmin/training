=========================
WiredTiger Storage Engine
=========================


Learning Objectives
-------------------

Upon completing this module, students should understand:

- The basic features of WiredTiger
- How the WT cache works
- How WT uses checkpoints and the journal to write to disk
- The basics of WT's compression and encryption
- How WT implements indexes

.. include:: /includes/student-notes.rst


What is WiredTiger?
-------------------

- An open source, high performance storage engine
- From the team that created BerkeleyDB
- Built to minimize contention between threads/processes
- Designed to minimize disk I/O, fully utilize CPU resources
- Introduced as a MongoDB storage engine in 3.0

  - Became the default in MongoDB 3.2

.. include:: /includes/student-notes.rst


Features of WiredTiger
----------------------

- Multi-Version Concurrency Controls (MVCC)

  - Keeps old versions of data while creating new ones
  - No locks; read/write threads see data as it existed when the thread started

- Different representations of data on-disk vs. in-memory

  - On-disk compression (and encryption with Enterprise)

- Indexes implemented just like collections

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - Indexes really are *almost* just like collections.
    - Minor differences:

      - They benefit from prefix compression (we'll see more in a few slides)
      - Just index key and an ObjectId; not the full BSON document

        - So they're smaller
      

Concurrency in WT
-----------------

- Updates don't happen in-place; a new copy is made

  - Old copies remain as long as necessary

- Reads/writes see only data commited before they begin
- If writes conflict, only one will "win"

  - The "losing" thread will start over, but cannot commit

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note:: 

    - This is all conceptual for now; that's OK
    - These statements apply not only to documents, but also to indexes
    - Because a "losing" write doesn't get committed, there is no need to "delete" it; you just start over fresh immediately


Writes in WT
------------

- When writes happen: 

  - The new version of the document is first prepared 
  
    - This takes a little time

  - In a single CPU cycle, the write gets committed

- Even two adjacent documents, getting updated simultaneously, won't conflict
- If writes do conflict, only one succeeds

  - The other write will will be retried with back-off

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note:: 

    - by "a little time", the exact time depends on:

      - Size of the document
      - Free CPU resources
      - % of cache that is dirty
      - etc.
      - The important thing is that the time it takes may vary, but its commit is essentially instantaneous

    - Writes will only conflict if they affect the exact same document

      - No practical way to predict which write will succeed in most cases


Introduction to the WiredTiger Cache
------------------------------------

- The WT cache is separate from the filesystem cache 

  - Their sum is limited by available RAM

- Greater of either 1 GB, or 60% of RAM minus 1 GB
- MongoDB writes to, and reads from, the WT cache
- The filesystem cache routes data from WT to disk and back

  - The FS cache *can* be an I/O bottleneck

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - In 3.0, WT cache size was the greater of 50% of RAM or 2 GB
    - WT cache size is tunable
    - FS cache will have to thrash a lot when reading/writing if it's too small

      - There's a reason why the default FS cache is about as big as the WT cache.


WT Data Structures - Pages
--------------------------

- Pages of documents are kept in b+tree structures 

  - One tree per collection
  - Many pages per tree

- Root page points to internal pages
- Internal pages point to leaf pages

  - Can also point to other internal pages

- Leaf pages contain the data (i.e., documents)

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note:: 

    - This isn't quite a simple b+tree:
    - All data is in the leaf nodes (like a b+tree)

      - But the node contains a bunch of unordered data, plus some metadata

        - But it contains a little index to get to that data


WT Data Structures - Pages (Continued)
--------------------------------------

- Pages are ordered and grouped by recordId
- RecordId is an internal identifier, *not* the _id!

    - recordId is used for the MongoDB Storage Engine API 

      - used by WiredTiger and MMAPv1

    - _id is a MongoDB value

  - In WT, it's a unique identifier (and is immutable)
  - In MMAPv1, it was a file offset (and mutable)

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note:: 

    - `cursor.showRecordId()` will display the recordId in documents


WT Data on Disk
---------------

- Data is organized into pages, just like in the cache

  - Root, internal, and leaf pages

- Each leaf page has:

  - A tiny header
  - An unordered series of key: value pairs

- Each page may be compressed
- Each page may be encrypted

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - This section is to help them understand pages in the cache

      - The cache is simpler

    - Compression / encryption is determined when the mongod is launched
    - Encryption at rest is only for MongoDB Enterprise
    
      - Sometimes called the "Encrypted Storage Engine," but it's not a new storage engine, it's just WT with additional functionality
     

WT Cache - Pages
----------------

- 3 parts:

  - Copy of the on-disk page (read only)
  - A small index to find the keys on the page 
  
    - Built when the page is read from disk

  - A skip list for writes (this can grow quite large)

- Writes do not cause pages to split (yet)

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - There is a picture of a tree on the next page.
    - The on-disk page is decompressed (if compression is used)
    
      - and unencrypted (if encryption is used)

WT Cache - Trees
----------------

.. figure:: /figures/trees-in-cache.png
   :align: center
   :figwidth: 700px

.. only:: instructor

  .. note::

    - Pages will split during reconciliation (it's the next slide)
    - Internal pages are 2K
    - Leaf (data bearing) pages are 32K, but can grow to accommodate large documents

      - Max of 16 MB, imposed by MongoDB


WT Cache -- Reconciliation
--------------------------

- Reconciliation is WT's process of incorporating write logs into the page
- Happens before data can get written to disk
- Documents in the page may need to be rewritten/inserted/deleted
- WT may need to split the page, if it grows too large

.. include:: /includes/student-notes.rst


WT Cache -- Reconciliation (Continued)
--------------------------------------

.. figure:: /figures/wt-page-reconciliation.png
   :align: center
   :figwidth: 650px

.. include:: /includes/student-notes.rst


Checkpoints
-----------

- WT's primary method of writing data to disk
- WT reconciles writes, makes clean pages
- A consistent snapshot of the data gets created, stored
- A new checkpoint does not delete the old 

  - Old pages get freed up later, in the background

- Starts 60 seconds after the end of the last checkpoint

  - Or 2 GB of writes

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - The journal should be used to write to disk between checkpoints
    - "Clean" pages in this context means that they have an empty update list

      - Everything gets incorporated into the "on disk" page
      - Updates going forward will make it into the update list


Checkpoints and Performance
---------------------------

- Checkpoints are non-blocking operations

  -  Still quite resource intensive

- Lots of I/O (compression mitigates this)
- Lots of CPU cycles (compression adds to this)
- Can see a performance hit during checkpoints

  - Especially for very large WT caches

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - Very large WT caches means around 100 GB+
    - Ironically, high write throughput between checkpoints can mean a bigger hit to performance during checkpoints

Checkpoints - Old and New Pages
-------------------------------

.. figure:: /figures/checkpoint-free-up-pages.png
   :align: center
   :figwidth: 700px

.. include:: /includes/student-notes.rst


Checkpoints - Old and New Pages 2
---------------------------------

.. figure:: /figures/checkpoint-free-up-pages-2.png
   :align: center
   :figwidth: 700px

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - This is the second of two images on checkpoints.


WT and the Journal
------------------

- Journal is *on* by default
- A complete view of data is captured on disk at the last checkpoint 
- Writes get logged to the journal between checkpoints
- The journal is compressed (Snappy by default)
- The journal in WT writes to disk every 50 ms

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note:: 

    - WT journal commit interval was 100 ms in 3.0
    - A separate thread uses shared memory to do the writes, so log buffers will be consolidated onto a single I/O process.

      - This is how multiple threads can all get their journal writes in at the same time

    - You can use the same library options (snappy, zlib, none) for journal compression as for block storage

      - You can use different options simultaneously for journal and block storage

    - Multiple threads writing to the journal can piggy back onto the same write to disk


Pitfall: WT Without the Journal
-------------------------------

- Don't use WiredTiger without the journal
- Turning off the journal will degrade performance with ``{ j : true }``

  - If the server must write to disk to acknowledge writes, it will begin a checkpoint
  - The overhead of the frequent checkpoints will far exceed the gains you would get by running without journaling
  - Remember, with replication protocol 1, ``{ w : "majority" }`` implies ``{ j : true }``

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - It's probably a good idea to remind the students that the "j" parameter will work even if the journal is off

      - It just means that you must flush to disk before acknowledging
      - For WiredTiger, this means a checkpoint is forced


Compression in WT
-----------------

- Applies to pages on disk
- Uses the Snappy library, by default

  - Good compression, low overhead

- zlib: better compression, but more CPU

- No compression is an option, too

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note:: 

    - We currently cap compression with zlib at 10x in order to prevent the CPU from working too long at compression/decompression
    - zlib is basically the same algorithm as gzip
    - Snappy is smart enough to recognize when it's dealing with pre-compressed data, and back off quickly
    - Some workloads achieve 10x compression


Encryption in WT
----------------

- Encryption at rest is only for MongoDB Enterprise, with WT

  - Complies with security, privacy requirements

- Encrypted only on-disk; not in RAM

  - Data is not natively encrypted over the wire 
  
    - Can use TLS/SSL (covered in another section)

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - Encryption ensures compliance with security and privacy standards required for some businesses
    - Sometimes called the "Encrypted Storage Engine," but it's not a new storage engine
    - For more details, see: https://docs.mongodb.org/manual/core/security-encryption-at-rest/#encrypted-storage-engine


Indexes in WT
-------------

- Implemented in the same data structures as collections

  - Same compression on-disk
  - Smaller b/c just keys are stored

- Updated only: 

  - A document gets inserted/deleted
  - When the indexed field is updated
  - This is in contrast to MMAPv1

.. include:: /includes/student-notes.rst


Indexes in WT: Prefix Compression
---------------------------------

- Indexes in memory can use prefix compression
- For in-memory page, WT stores the common prefix *once*

  - Works because keys in indexes are ordered

- Unlike block compression, doesn't eat CPU cycles

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - `cursor.showRecordId()` will display the recordId in documents
    - Index prefix compression:

      - It's called "compression," but it's just about storing the common prefix once per page.
      
        - often can end up using *fewer* cpu cycles

    - Students may ask about Log Structured Merge (LSM) trees.

      - LSM trees are present natively in WT, but not implemented for MongoDB, yet.
      - LSM trees would allow updates to indexes to happen smoothly in the background during heavy write loads

        - Allows you to build additional tree structures to avoid requent rebalancing (during a bulk load, for example)
