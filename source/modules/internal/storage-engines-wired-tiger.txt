=========================
WiredTiger Storage Engine
=========================


Learning Objectives
-------------------

Upon completing this module, students should understand:

- The basic features of WiredTiger
- How the WT cache works
- How WT uses checkpoints and the journal to write to disk
- The basics of WT's compression and encryption
- How WT implements indexes
- Main issues with WT

.. include:: /includes/student-notes.rst


What is WiredTiger?
-------------------

- An open source, high performance storage engine
  - From the team that created BerkeleyDB
- Built to minimize contention between threads/processes
- Designed to minimize disk I/O, fully utilize CPU resources
- Introduced as a MongoDB storage engine in 3.0
- WiredTiger became the default in MongoDB 3.2

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - [DRAW] WT as a subsystem of MongoDB (TODO provide the diagram for the instructor)

      - show that the feature of Wired Tiger transaction is used by MongoDB
        to commit changes to data and indexes together, not exposing
        transactions to the MongoDB user.


Features of WiredTiger
----------------------

- Multi-Version Concurrency Controls (MVCC)

  - Keeps old versions of data while creating new ones
  - No locks; read/write threads see data as it existed when the thread started

- Different representations of data on-disk vs. in-memory
- On-disk compression (and encryption with Enterprise)
- Indexes implemented just like collections

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - Indexes really are *almost* just like collections.
    - Minor differences:

      - They benefit from prefix compression (we'll see more in a few slides)
      - Just index key and an ObjectId; not the full BSON document

        - So they're smaller


Concurrency in WT
-----------------

- Updates don't happen in-place; a new copy is made
- Old copies remain as long as necessary
- Reads/writes see only data committed before they begin
- If writes conflict, only one will "win"
- The "losing" thread will start over, but cannot commit

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - This is all conceptual for now; that's OK
    - These statements apply not only to documents, but also to indexes
    - Because a "losing" write doesn't get committed, there is no need to "delete" it; you just start over fresh immediately

.. only:: todo

   - clarify "but cannot commit"


Writes in WT
------------

- When writes happen:

  - The new version of the document is first prepared
  - This takes a little time
  - In a single CPU cycle, the write gets committed

- Even two adjacent documents, getting updated simultaneously, won't conflict
- If writes do conflict, only one succeeds
- The other write will will be retried with back-off

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - by "a little time", the exact time depends on:

      - Size of the document
      - Free CPU resources
      - % of cache that is dirty
      - etc.
      - The important thing is that the time it takes may vary, but its commit is essentially instantaneous

    - Writes will only conflict if they affect the exact same document

      - No practical way to predict which write will succeed in most cases

    - Application must handle logic to avoid undesired parallel changes.
      Can use a version number (or timestamp) that is passed back to the update
      This is not specific to Wired Tiger, but the same for MMAPv1


Introduction to the WiredTiger Cache
------------------------------------

.. rst-class:: longlist

- The WT cache is separate from the filesystem cache

  - Their sum is limited by available RAM

- WT cache size:

  - in 3.4, greater of either 256 MB, or 50% of RAM minus 1 GB
  - in 3.2, greater of either 1 GB, or 60% of RAM minus 1 GB
  - in 3.0, was the greater of 50% of RAM or 2 GB

- MongoDB writes to, and reads from, the WT cache
- MongoDB compress/uncompress pages from the WT cache to the file system cache
- The filesystem cache routes data from the disk to the FS cache and back
- The FS cache *can* be an I/O bottleneck

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - WT cache size is tunable

      - it is still not clear what is the optimal size for the cache
      - show example of large WT cache, small FS cache vs small WT cache/large FS cache

    - FS cache will have to thrash a lot when reading/writing if it's too small
    - There's a reason why the default FS cache is about as big as the WT cache.
    - One TSE opinion: if your uncompressed working set fits in memory, then make the WiredTiger Cache large (up to 60%); if your compressed working set fits in memory, then make the fscache large (up to 90%);


WT Cache Size - Some Guidelines
-------------------------------

.. rst-class:: longlist

- Can't rely on Page Faults, so look at ``iostats`` and ``serverStatus()`` metrics:

  - disk reads
  - disk writes
  - bytes read into cache
  - bytes written from cache

- If **low** *disk reads* + **high** *bytes read into cache*

  - increase *WT Cache* size

- If **medium to high** *disk reads* + **medium to high** *bytes read into cache*

  - add RAM

- If **medium to high** *disk reads* + **low to medium** *bytes read into cache*

  - decrease the *WT cache* size


WT Data Structures - Pages
--------------------------

- Pages of documents are kept in b+tree structures

  - One tree per collection
  - Many pages per tree

- Root page points to internal pages
- Internal pages point to leaf pages
- Can also point to other internal pages
- Leaf pages contain data (i.e., documents)

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - This isn't quite a simple b+tree:
    - All data is in the leaf nodes (like a b+tree)

      - But the node contains a bunch of unordered data, plus some metadata

        - But it contains a little index to get to that data


WT Data Structures - Pages (Continued)
--------------------------------------

- Pages are ordered and grouped by *recordId*
- *recordId* is an internal identifier, *not* the _id
- *recordId* is used for the MongoDB Storage Engine API
- In WT, it's a unique identifier (and is immutable)
- In MMAPv1, it was a file offset (and mutable)

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - [ASK] can you tell what is the problem with mutable values for MMAPv1

      - should have been covered in the index section

    - `cursor.showRecordId()` will display the recordId in documents


WT Data on Disk
---------------

- Data is organized into pages, just like in the cache
- Root, internal, and leaf pages

- Each leaf page has:

  - A tiny header
  - An unordered series of key: value pairs

- Each page may be compressed
- Each page may be encrypted

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - This section is to help them understand pages in the cache
    - The cache is simpler
    - Compression / encryption is determined when the mongod is launched
    - Encryption at rest is only for MongoDB Enterprise
    - Sometimes called the "Encrypted Storage Engine," but it's not a new storage engine, it's just WT with additional functionality


WT Cache - Pages
----------------

- 3 parts:

  - Copy of the on-disk page (read only)
  - A small index to find the keys on the page built when the page is read from disk
  - A skip list for writes (this can grow quite large)

- Writes do not cause pages to split

.. include:: /includes/student-notes.rst


WT Cache - Trees
----------------

.. include:: /modules/trees-in-cache.txt

.. only:: instructor

  .. note::

    - Pages will split during reconciliation (see next slide)
    - Internal pages are 2K
    - Leaf (data bearing) pages are 32K, but can grow to accommodate large documents
    - Max of 16 MB, imposed by MongoDB

.. include:: /includes/student-notes.rst


WT Cache -- Reconciliation
--------------------------

- Reconciliation is WT's process of incorporating write logs into the page
- Happens before data can get written to disk
- Documents in the page may need to be rewritten/inserted/deleted
- WT may need to split the page, if it grows too large

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - WT will search for available space (closer larger page) in the former "old pages", that are now marked for re-use. It will not carve a block of 32KB out of a 1M page for example
    - Compaction can happen concurrently with other operations, of course, but compaction
      is a relatively heavy-weight operation, and can significantly increase I/O while
      decreasing overall throughput in I/O bound workloads.


WT Cache -- Reconciliation (Continued)
--------------------------------------

.. include:: /figures_meta/wt-page-reconciliation.txt

.. include:: /includes/student-notes.rst


Checkpoints
-----------

- WT's primary method of writing data to disk
- WT reconciles writes, makes clean pages
- A consistent snapshot of the data gets created, stored
- A new checkpoint does not delete the old one
- Old pages get freed up later, in the background
- Starts 60 seconds after the end of the last checkpoint or 2 GB of writes

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - The journal should be used to write to disk between checkpoints
    - "Clean" pages in this context means that they have an empty update list

      - Everything gets incorporated into the "on disk" page
      - Updates going forward will make it into the update list


Checkpoints and Performance
---------------------------

- Checkpoints are non-blocking operations but are resource intensive
- Lots of I/O (compression mitigates this)
- Lots of CPU cycles (compression adds to this)
- Can see a performance hit during checkpoints, especially for very large WT caches

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - Very large WT caches means around 100 GB+
    - High write throughput between checkpoints can mean a bigger hit to performance during checkpoints


Checkpoints - Old and New Pages
-------------------------------

.. include:: /figures_meta/checkpoint-free-up-pages.txt

.. include:: /includes/student-notes.rst


Checkpoints - Old and New Pages 2
---------------------------------

.. include:: /figures_meta/checkpoint-free-up-pages-2.txt

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - This is the second of two images on checkpoints.


WT and the Journal
------------------

- Journal is *on* by default
- A complete view of data is captured on disk at the last checkpoint
- Writes get logged to the journal between checkpoints
- The journal is compressed (Snappy by default)
- The journal in WT writes to disk every:

  - 50 ms in 3.2 and 3.4
  - 100 ms in 3.0

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - A separate thread uses shared memory to do the writes, so log buffers will be consolidated onto a single I/O process.

      - This is how multiple threads can all get their journal writes in at the same time

    - You can use the same library options (snappy, zlib, none) for journal compression as for block storage

      - You can use different options simultaneously for journal and block storage

    - Multiple threads writing to the journal can piggy back onto the same write to disk


Pitfall: WT Without the Journal
-------------------------------

- Don't use WiredTiger without the journal
- Turning off the journal will degrade performance with ``{ j : true }``

  - If the server must write to disk to acknowledge writes, it will begin a checkpoint
  - The overhead of the frequent checkpoints will far exceed the gains you would get by running without journaling
  - Remember, with replication protocol 1, ``{ w : "majority" }`` implies ``{ j : true }``

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - It's probably a good idea to remind the students that the "j" parameter will work even if the journal is off

      - It just means that you must flush to disk before acknowledging
      - For WiredTiger, this means a checkpoint is forced

.. only:: todo

   - does WT reduce the interval of the journal with ``{ j : true }`` for an operation, like MMAPv1 does?


Compression in WT
-----------------

- Applies to pages on disk
- Uses the Snappy library, by default

  - Good compression, low overhead

- zlib: better compression, but more CPU

- No compression is an option, too

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - We currently cap compression with zlib at 10x in order to prevent the CPU from working too long at compression/decompression
    - zlib is basically the same algorithm as gzip
    - Snappy is smart enough to recognize when it's dealing with pre-compressed data, and back off quickly
    - Some workloads achieve 10x compression


Encryption in WT
----------------

- Encryption at rest is only for MongoDB Enterprise, with WT

  - Complies with security, privacy requirements

- Encrypted only on-disk; not in RAM

  - Data is not natively encrypted over the wire

    - Can use TLS/SSL (covered in another section)

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - Encryption ensures compliance with security and privacy standards required for some businesses
    - Sometimes called the "Encrypted Storage Engine," but it's not a new storage engine
    - For more details, see: https://docs.mongodb.org/manual/core/security-encryption-at-rest/#encrypted-storage-engine


Indexes in WT
-------------

- Implemented in the same data structures as collections

  - Same representation on-disk and in memory
  - Smaller because just values are stored (no need to save key names)

- Updated only:

  - A document gets inserted/deleted
  - When the indexed field is updated
  - This is in contrast to MMAPv1

.. include:: /includes/student-notes.rst


Indexes in WT: Prefix Compression
---------------------------------

- Indexes in memory can use prefix compression
- For in-memory page, WT stores the common prefix *once*

  - Works because keys in indexes are ordered

- Unlike block compression, doesn't eat CPU cycles

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

    - `cursor.showRecordId()` will display the recordId in documents
    - Index prefix compression:

      - It's called "compression," but it's just about storing the common prefix once per page.

        - often can end up using *fewer* cpu cycles

    - Students may ask about Log Structured Merge (LSM) trees. We don't want to spend too much time on the subject as they are not planned in any release any time soon.

      - LSM trees are present natively in WT, but not implemented for MongoDB, yet. We tried to use them, but got a lot of issues, and backed off on their usage.

      - LSM trees would allow updates to indexes to happen smoothly in the background during heavy write loads

        - Allows you to build additional tree structures to avoid requent rebalancing (during a bulk load, for example)

.. only:: todo

   - screenshot showing a WiredTiger leaf page with index values, using prefix compression


Other storage engines based on WiredTiger
-----------------------------------------

- Encrypted
- In-Memory
- Future ones? Columnar?

.. include:: /includes/student-notes.rst

.. only:: instructor

  .. note::

     - not many customers use the in-Memory storage engine, here are some...

       - MyDealerLot


Situations where MMAPv1 Performs better than WiredTiger
-------------------------------------------------------

- Frequent small updates on large documents

- Latency


Situations where WiredTiger Performs Poorly
-------------------------------------------

- Capped collections in MongoDB 3.0

- Large number of collections being created/destroyed

  - WT has to touch the disk to create files each time

- Running without journaling but requiring acknowledgment of writes to disk

  - Frequent checkpoints

- Compression performs poorly with pre-compressed files

  - e.g. image files

.. include:: /includes/student-notes.rst


Main issues seen with WiredTiger
--------------------------------

- MongoDB 3.0 would exhibit stalls under high load. Main culprits were:

  - sub optimal implementation for *capped collections*
  - issues when creating checkpoints

- MongoDB 3.2 also would have performance issues. Main culprit is:

  - eviction of pages from the WT cache

- Usually, should not colocate 2 WT process on the same machine

- Should the *read ahead* setting be 0?

.. only:: instructor

  .. note::

    - up to 80% cache fulled, no eviction is needed
    - at 80%, eviction starts, by 4 thread
    - at 95%, the application threads also evict data, not processing requests anymore


Troubleshooting WiredTiger
--------------------------

- data collection:

  - 3.0, scripts to run 'serverStatus' and 'iostats' for a period of time
  - 3.2, ``diagnostic.data`` directory, located in the *dbpath*

- data analysis

  - `Bruce Lucas' life saving tool <https://github.com/10gen/support-tools/blob/master/timeseries/quickstart.md>`_

.. only:: instructor

  .. note::

    - number of tickets (128 available) go to zero

    - increasing won't help, there is a hardware bottleneck

.. only:: instructor

  .. note::

    - [ACTION] Go to Bruce's page and show some of the time series graph
    - *tickets* can be seen as *threads*


References on WiredTiger
------------------------

- `WiredTiger for Dummies by TSEs <https://docs.google.com/document/d/1LRjiYGLYTIIg7zX4U2GNezFLqplAplYfqYDR-1nVwvc/edit?ts=5810b121>`_, 11/2016

  - There is a reference section at the end

- Nasty WT bugs:

  - SERVER-24580
  - SERVER-20306
  - SERVER-21275
  - SERVER-23425

.. only:: instructor

  .. note::

    [Ask] closing questions

    - If you had a standalone WT instance, how much data can we lose if it crashes?

      - how about for a replica set
